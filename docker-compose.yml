services:
  # DATABASE A!!!
  mongo:
    image: mongo:6
    container_name: mongo
    restart: always
    # volumes:
    #   - mongo_data:/data/db
    expose:
      - "27017" # Exposed to other containers in the same network
    networks:
      - pipeline_network

  mongo-express:
    image: mongo-express
    container_name: mongo_web
    restart: always
    environment:
      ME_CONFIG_MONGODB_SERVER: mongo
      ME_CONFIG_BASICAUTH_USERNAME: ${MONGO_INITDB_ROOT_USERNAME}
      ME_CONFIG_BASICAUTH_PASSWORD: ${MONGO_INITDB_ROOT_PASSWORD}
    ports:
      - "8081:8081" # Only this is available to your host machine
    depends_on:
      - mongo
    networks:
      - pipeline_network

  # DATABASE B!!!
  mysql:
    image: mysql:8
    container_name: mysql
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
      MYSQL_DATABASE: ${MYSQL_DATABASE}
      MYSQL_USER: ${MYSQL_USER}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD}
    # expose:
    #   - "3306" # Exposed to other containers in the same network
    ports:
      - "3306:3306" # Only this is available to your host machine
    networks:
      - pipeline_network

  adminer:
    image: adminer
    container_name: adminer
    restart: always
    ports:
      - "8080:8080" # Only this is available to your host machine
    depends_on:
      - mysql
    networks:
      - pipeline_network

  minio:
    image: minio/minio
    container_name: minio
    ports:
      - "9000:9000" # S3 API
      - "9090:9090" # Web UI
    environment:
      MINIO_ROOT_USER: ${MINIO_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_PASSWORD}
    command: server /data --console-address ":9090"
    networks:
      - pipeline_network

  postgres_analytics:
    image: postgres:13
    container_name: postgres_analytics
    restart: always
    environment:
      POSTGRES_USER: ${DATA_ANALYST_USER}
      POSTGRES_PASSWORD: ${DATA_ANALYST_USER}
      POSTGRES_DB: analytics
    expose:
      - "5432"
    # volumes:
    #   - airflow_postgres_data:/var/lib/postgresql/data
    networks:
      - pipeline_network

  postgres_airflow:
    image: postgres:13
    container_name: postgres_airflow
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_AIRFLOW_USER}
      POSTGRES_PASSWORD: ${POSTGRES_AIRFLOW_PASSWORD}
      POSTGRES_DB: airflow
    expose:
      - "5432"
    networks:
      - pipeline_network

  airflow-webserver:
    image: apache/airflow:2.8.1
    container_name: airflow_web
    restart: always
    depends_on:
      - postgres_airflow
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_AIRFLOW_USER}:${POSTGRES_AIRFLOW_PASSWORD}@postgres_airflow:5432/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: mysecretkey
      AIRFLOW__CORE__FERNET_KEY: ""
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt_project:/opt/airflow/dbt_project
      - ./.dbt:/home/airflow/.dbt

    ports:
      - "10000:8080"
    command: webserver
    networks:
      - pipeline_network

  airflow-scheduler:
    image: apache/airflow:2.8.1
    container_name: airflow_scheduler
    restart: always
    depends_on:
      - postgres_airflow
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_AIRFLOW_USER}:${POSTGRES_AIRFLOW_PASSWORD}@postgres_airflow:5432/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt_project:/opt/airflow/dbt_project
      - ./.dbt:/home/airflow/.dbt

    command: scheduler
    networks:
      - pipeline_network

  airflow-init:
    image: apache/airflow:2.8.1
    container_name: airflow_init
    depends_on:
      - postgres_airflow
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_AIRFLOW_USER}:${POSTGRES_AIRFLOW_PASSWORD}@postgres_airflow:5432/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./dbt_project:/opt/airflow/dbt_project
      - ./.dbt:/home/airflow/.dbt

    command: bash -c "airflow db migrate && airflow users create \
      --username ${AIRFLOW_USER} \
      --firstname Ibrahim \
      --lastname Fikry \
      --role Admin \
      --email admin@example.com \
      --password ${AIRFLOW_PASSWORD}"
    networks:
      - pipeline_network

  metabase:
    image: metabase/metabase:latest
    container_name: metabase
    ports:
      - "3000:3000"
    environment:
      MB_DB_TYPE: postgres
      MB_DB_DBNAME: metabase
      MB_DB_PORT: 5432
      MB_DB_USER: ${DATA_ANALYST_USER}
      MB_DB_PASS: ${DATA_ANALYST_PASSWORD}
      MB_DB_HOST: postgres_analytics
    depends_on:
      - postgres_analytics
    networks:
      - pipeline_network

networks:
  pipeline_network:
    driver: bridge
# volumes:
#   mongo_data:
